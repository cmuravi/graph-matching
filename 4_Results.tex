\section{Results}
\subsection{Fixed Degree Model}
\label{fixed-degree}

In this model, we assume that a bipartite graph $G=(L,R,E)$ is
generated probabilistically by the following procedure. Each
vertex $v\in L$, uniformly samples a set of $d$ neighbors
from $R$. For convenience let $|L|=l$, $|R|=r$ and $k=l/r$. From
$G$, we sample a subgraph $H$ where for each vertex $u\in L$ a set of
$c$ neighbors are sampled uniformly from set of incident vertices. The
following theorem derives a lower bound on the expected size $S$ of the
number $v\in R$ such that $\deg_H(v) \geq 1$.

\begin{thm}
Suppose that $G=(L,R,E)$ and $H\subseteq$ is generated as above. Then
\[ \E[S] \geq r(1-\exp(-ck))\]
where the expectation is over the random sampling of $G$ and $H$.
\end{thm}
\begin{proof}
For each $v\in R$ let $X_v$ be the indicator variable for the event
that $\deg_H(v) \geq 1$. Note that since for each vertex $u\in L$, $H$
uniformly samples from a uniformly sampled set of neighbors, we can
think $H$ as being generated by the same process that generated $G$,
but with $d$ replaced with $c$. Now for a specific vertex $u \in R$,
the probability that it has no incident edges is 
$\left(1-\frac{1}{r}\right)^c$. Since the selection of neighbors for each 
vertex in $L$ is independent, it follows that that:
\[ \Pr[X_v=0] = \left(1-\frac{1}{r}\right)^{cl} \leq \exp\left(-c \cdot \frac{l}{r}\right) = \exp(-ck) \]
Note that $S = \sum_{v\in R} X_v$. Applying linearity of expectation, we get
\[ \E[S] = \sum_{v\in V} \E[X_v] \geq r(1-\exp(-ck))\]
\end{proof}

While this shows a lower bound in absolute terms, we must compare it to the best possible solution {\em OPT}. The follow theorem proves the approximation ratio to {\em OPT}.

\begin{thm}
The above sampling algorithm gives a $1-1/e$ factor approximation to the $(c,1)$-graph recommendation problem in expectation
\end{thm}
\begin{proof}
The size of the optimal solution is bounded above by both the number
of edges in the graph and the number of vertices in $R$. The former of
these is $cl=ckr$ and the latter is $r$, which shows that $OPT \leq
r\max(ck,1)$. Therefore, by simple case analysis the approximation ratio in
expectation is at least
\[ \frac{1-\exp(-ck)}{\min(ck,1)} \geq 1-\frac{1}{e} \]
\end{proof}

However in reality, the approximation obtained by this sampling
approach can be much better for certain values of $ck$. In particular,
if $ck>1$, then the approximation ratio is $1-\exp(-ck)$, which
approaches 1 as $ck\to\infty$. In particular, if $ck=3$, then the
solution will be at least 95\% as good as the optimal solution even
with our trivial bounds. Similarly, when $ck<1$, the approximation
ratio is $(1-\exp(-ck))/ck$ which also approaches 1 as $ck\to 0$. In
particular, if $ck=0.1$ then the solution will be at 95\% as good as
the optimal solution. The case when $ck=1$ therefore represents the
worst case outcome for this model where we only guarantee 63\%
optimality. The graph below shows the approximation ratio as a
function of $ck$.

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{Sri_Original.png}
\caption{Approximation Ratio as a function of $ck$ }
\end{figure}

Now suppose that $G$ is generated and $H$ is sampled using the same
processes as described above. In the next theorem, we extend the above
bounds to the $(c,a)$-graph recommendation problem where $a>1$. 
%In particular if we set $a=1$, we will obtained the estimate from the original analysis.

\begin{thm}
Let $S$ be the random variable denoting the number of vertices $v \in R$ such that $\deg_{H}(v)\geq a$. Then
\[ \emph{\E}[S] \geq r\left(1-e^{-ck+\frac{a-1}{r}}\frac{(ck)^a-1}{ck-1}\right)  \]
where the expectation is over the randomness of $G$ and $H$.
\end{thm}

\begin{proof}
Let $X_{uv}$ be the indicator variable of the event that the edge $uv$
(note that $u\in L$ and $v\in R$) is in the subgraph that we picked
and set $X_{v} = \sum_{u\in U} X_{uv}$ so that $X_{v}$ represents the
random degree of the vertex $v$ in our subgraph. Because our algorithm
uniformly subsamples a uniformly random selection of edges, we can
assume that$H$ was generated the same way as $G$ but sampled $c$
instead of $d$ edges for each vertex $u\in L$. So $X_{uv}$ is a
bernoulli random variable. Using the trivial bound $\binom{n}{i}
\leq n^i$ on binomial coefficients we get:
\begin{align*}
      \Pr[X_v < a]
&=    \sum_{i=0}^{a-1} \binom{cl}{i} \left(1-\frac{1}{m}\right)^{cl-i}\left(\frac{1}{r}\right)^i \\
&\leq \sum_{i=0}^{a-1} \left(\frac{cl}{r}\right)^i\left(1-\frac{1}{r}\right)^{cl-i} \\
&=    \left(1-\frac{1}{r}\right)^{cl-(a-1)}\sum_{i=0}^{a-1} (ck)^i \\
&\leq \left(1-\frac{1}{r}\right)^{cl-(a-1)}\frac{(ck)^a-1}{ck-1} \\
&\leq e^{-ck+\frac{a-1}{r}} \frac{(ck)^a-1}{ck-1}
\end{align*}


Letting $Y_v = \left[X_v \geq a\right]$, we now see that

\[ \E[S] = \E\left[\sum_{v\in R} Y_v\right] \geq r\left(1-e^{-ck+\frac{a-1}{r}} \frac{(ck)^a-1}{ck-1}\right) \]
\end{proof}

Now, we can perform a similar analysis as in the previous section. In
particular, if $ck>a$, then the problem is easy on average though we
need $ck$ to get larger than before to be close to optimal. This
is in comparison to the trivial estimate of $m$. For a fixed $a$, a
random solution gets better as $ck$ increases because the decrease in
$e^{-ck}$ more than compensates for the polynomial in $ck$ next to
it. However, if $ck<a$, we need to use the trivial estimate of
$ckr/a$, and the analysis from the previous section does not extend
here. \\

In both this section and the previous one, $ck$ is the average degree
of a vertex $v\in R$ in our chosen subgraph. Basically, what the
original analysis said is that if $ck>1$, then the sampling algorithm
will probably cover every vertex in $R$ since the expected degree of
each vertex is large. On the other hand if $ck$ is small ($ck < 1$)
then the best possible solution is obtained when none of the vertices
in $R$ has degree greater than 1. If $ck<1$, then we do not cover very
many vertices in $R$, but we also do not cover many vertices more than
once. Since the optimal solution in this case was correspondingly low,
our solution was good in the $a=1$ case. However, when $a<1$, the fact
that our edges are well-dispersed only hurts our solution because we
need to concentrate the edges on particular $v\in R$ that will
count. The following table shows how large $ck$ needs to be for the
solution to be 95\% optimal for different values of $a$:

\begin{figure}[h]
  \centering
  \begin{tabular}{ |c|c|c|c|c|c| }
    \hline
    $a$ & 1 & 2 & 3 & 4 & 5 \\ \hline
    $ck$ & 3.00 & 4.74 & 7.05 & 10.01 & 13.48 \\
    \hline
  \end{tabular}
  \caption{The required $ck$ to obtain 95\% optimality}
\end{figure}

\subsection{Hierarchical Tree Model}
\label{hierarchy}
In this section we explore the hierarchical tree model. We will assume
that we are given a bipartite graph $G=(L,R,E)$. The vertex sets $L$
and $R$ are the leaf sets of two full binary trees $T_L$ and $T_R$ of
depth $D$ where there is a one-to-one correspondence between the
subtrees of these two trees. We also assume that each branching in
both $T_L$ and $T_R$ splits the nodes evenly into the two
subtrees. As in the previous sections, we set $|L|/|R|=k$, but we also
note that this ratio still holds if we take any subtree on the left 
and its corresponding subtree on the right. By abuse of notation, we 
will use a subtree and its leaf set interchangeably. The trees are
fixed in advance, but $G$ is generated probabilistically according to
the following procedure. Let $u\in L$ and $T_L^0, \ldots T^{D-1}_L$ be
the subtrees it belongs at depths $0,\ldots, D-1$. Also, let 
$T_R^0,\ldots, T_R^{D-1}$ be the subtrees on the right that correspond
to these trees on the left. We let $u$ make an edge to $d_{D-1}$ of
the vertices in $T_{R}^{D-1}$, $d_{D-2}$ edges to the vertices in 
$T_{R}^{D-1} \backslash T_{R}^{D-2}$ and so on. Each vertex is chosen
with uniform probability and we let $d = d_{0} + \ldots + d_{D-1}$.\vs

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{hierarchy_tree.png}
\begin{minipage}[h]{0.7\textwidth}
\caption{This diagram shows the notation we use for this model and the 1-to-1 correspondance of subtrees}
\end{minipage}
\end{figure}

Our goal now is to find a $b$-matching in this graph that is close to
optimal in expectation. That is, our degree upper bound on vertices in
$L$ is $c$ and the degree lower bound on vertices in $L$ is 1. Let $c
= c_0 + \ldots + c_{D-1}$ similarly to how we defined $d$. 
To combine the analysis of the randomness of the algorithm
and the randomness of the graph, the algorithm will pick $c_{i}$ edges
uniformly from among the $d_{i}$ edges going to each level of the
subtree. This enables us to think of the subgraph our algorithm
outputs as being generated by the graph generation process, but with
fewer neighbors selected for each as in the previous sections. With
this model and parameters in place, we can prove the following
theorem.\

\begin{thm}
Let $S$ be the subset of edges $v\in R$ such that $\deg_H(v) \geq 1$. Then
\[ \E[S] \geq r(1-\exp(-ck)) \]
where the expectation is taken over $G$ and $H$.
\end{thm}

\begin{proof}
Let $v\in R$ and let $T_L^{D-1}, T_L^{D-2}\backslash T_L^{D-1},
\ldots, T_L^0\backslash T_L^1$ be the sets it can take edges
from. Since $T_L$ and $T_R$ split perfectly evenly at each node the
vertices in these sets will be choosing from $r_{D-1}, r_{D-1},
r_{D-2},\ldots, r_{1}$ vertices in $R$ for neighbors respectively,
where $r_i$ is the size of subtree of the right tree rooted at depth
$i$. Furthermore, each of these sets described above have size
$l_{D-1}, l_{D-1}, l_{D-2}, \ldots, l_{1}$ respectively, where $l_i$
is the size of a subtree of $T_L$ rooted at depth $i$. It follows that
the probability that $v$ does not receive any edges at all is at most

\begin{align*}
	      \Pr[\lnot X_v] 
	&=    \left(1-\frac{1}{r_{D-1}}\right)^{c_0l_{D-1}}\prod_{i=1}^{D-1}\left(1 - \frac{1}{r_i}\right)^{c_{D-i} l_i} \\
	&\leq \exp\left(-\frac{l_{D-1}}{r_{D-1}}c_0\right)\prod_{i=1}^{D-1} \exp\left(-\frac{l_i}{r_i}c_{D-i}\right) \\
	&=    \exp\left(-(c_0 + \ldots + c_{D-1})k\right) \\
	&=    \exp(-ck)
\end{align*}

Since this is an indicator variable, it follows that 
\[ \E[S] = \E\left[\sum_{v \in R} X_v \right] \geq r \left(1-\exp(-ck)\right) \]
\end{proof}

Note that this is the same result as we obtained for the fixed degree
model in Section \ref{fixed-degree}. In fact, the approximation
guarantees when $ck \ll 1$ or $ck \gg 1$ hold exactly as before.\vs

The sampling of $H$ can be done algorithmically because we separated
out the edge generation process at a given depth from the edge
generation process at deeper subtrees. There is no ambiguity as to why
an edge is in the underlying graph. That is, if we superimpose $T_L$
and $T_R$, then an edge between $u_l\in L$ and $v_r\in R$ must have
come from an edge generated at the lowest common ancestor of $u_l$ and
$v_r$. So the algorithm can actually sample intelligently and in the
same way that the graph was generated in the first place. Also note
that we do not have to assume that the trees $T_L$ and $T_R$ are
binary. We only need the trees to be regular and evenly divided at
each vertex since the proof only relies on the proportions of the
sizes of the subtrees in $T_L$ and $T_R$.


\subsection{Cartesian Product Model}
\label{cartesian}
We can extend the analysis in Section \ref{fixed-degree} in a way
orthogonal to the hierarchical tree model as follows. We assume that
$L$ has been partitioned into $t$ subsets $L_1,\ldots, L_t$ and
that $R$ has been partitioned into $t'$ subsets $R_1,\ldots,
R_{t'}$. For convenience, we let $|L_i| = l_i$ and $|R_i|=r_i$. Given
this suppose that for each $1\leq i\leq t$ and each $1\leq j\leq t'$,
$G[L_i, R_j]$ is an instance of the Fixed Degree Model with
$d=d_{ij}$. We assume that for all $i$, we have $\sum_{j=1}^{t'}
d_{ij} = d$ for some fixed $d$. Also assume that we have fixed in
advance $c_{ij}$ for each $1\leq i\leq t$ and $1\leq j\leq t'$ that
satisfy $\sum_{j=1}^{t'} c_{ij} = c$ for all $i$ for some fixed $c$.
To sample $H$ from $G$, we sample $c_{ij}$ neighbors for each 
$u_i\in L_i$ from $R_i$. Letting $S$ be the set of vertices in 
$v\in R$ that satisfy $\deg_H(v)\geq 1$, we can show the following:

\begin{thm}
With $S$, $G$ and $H$ defined as above, we have
\[ \E[S] \geq r - \sum_{j=1}^{t'} r_j \exp\left(-\sum_{i=1}^t c_{ij} \frac{l_i}{r_j}\right)\]
where the expectation is over $G$ and $H$.
\end{thm}
\begin{proof}
Let $v_j \in R_j$ be an arbitrary vertex and let $X_{v_j}$ be the
indicator variable for the event that $\deg_H(v_i) \geq 1$. The
probability that none of the neighbors of some $u_i\in R_i$ is $v_j$
is exactly $(1-\frac{1}{r_j})^{c_{ij}}$. It follows that the
probability that the degree of $v_j$ in the subgraph $H[L_i,R_j]$ is 0
is at most $(1-\frac{1}{r_j})^{c_{ij}l_i}$. Considering this
probability over all $R_j$ gives us:
\[ \Pr[X_{v_i} = 0] = \prod_{i=1}^{t} \left(1-\frac{1}{r_j}\right)^{c_{ij} l_i} \leq \exp\left(-\sum_{i=1}^t c_{ij} \frac{l_i}{r_j}\right)\]

By linearity of expectation $\E[S] = \sum_{i=1}^{t'} r_i \E[X_{v_i}]$,
so it follows that
\[ \E[S] \geq \sum_{j=1}^{t'} r_j \left(1-\exp\left(-\sum_{i=1}^t c_{ij} \frac{l_i}{r_j}\right)\right) = r - \sum_{j=1}^{t'} r_j \exp\left(-\sum_{i=1}^t c_{ij} \frac{l_i}{r_j}\right)\]
\end{proof}

This model is interesting because it can capture a broader set of
recommendation subgraphs than the fixed degree model. However, it is
difficult to estimate how good a solution will be without knowing
the sizes of the sets in the partitions. However, we can note that we
obtain the approximation guarantee of $(1-\exp(-ck))$ provided that
$l_i/r_j = k$ for all $i$ and $j$ where $k$ is some fixed
constant. Another interesting point about this model and the algorithm
we described for sampling $H$ is that we are free to set the $c_{ij}$
as we see fit. In particular, $c_{ij}$ can be chosen to maximize the
approximation guarantee in expectation we obtained above using
gradient descent or some other first order method prior to running the
recommendation algorithm to increases the quality of the solution.


\subsection{Weighted Model}
\label{weighted}
The fixed degree model of Section \ref{fixed-degree} is a simple and
convenient model, but the assumption that all recommendations hold the
same weight is unrealistic. This motivates fixing the graph to be the
complete bipartite graph $K_{l,r}$, and giving the edges i.i.d weights
with mean $\mu$. We modify the objective function accordingly, so that
we count only the vertices in $R$ which have weight $\geq 1$. If we
assume that $ck\mu \geq 1+\epsilon$ for some $\epsilon > 0$, then 
naive the solution sampling solution we outlined in Section \ref{fixed-degree}
still performs exceptionally well. Letting $S$ be the size of the 
solution produced by this algorithm we have:

\begin{thm}
Let $G=K_{l,r}$ be a bipartite graph where the edges have i.i.d. weights and come from a distribution with mean $\mu$ that is supported on $[0,b]$. If the algorithm from Section \ref{fixed-degree} is used to sample a subgraph $H$ from $G$, then
\[ \E[S] = \sum_{v\in R} \E[X_v] = r\left(1-\exp\left(-\frac{2l\epsilon^2}{b^2}\right)\right) \]
\end{thm}

\begin{proof}
For each edge $uv\in G$, let $W_{uv}$ be its random weight, $Y_{uv}$ be
the indicator for the event $uv\in H$ and define $X_{uv} = Y_{uv}
W_{uv}$. Since weights and edges are sampled by independent processes,
we have $\E[X_{uv}] = \E[W_{uv}]\E[Y_{uv}]$ for all edges. Since $c$
edges out of $r$ are picked for each vertex, $\E[Y_{uv}] = \frac{c}{r}$
, so $\E[X_{uv}] = \frac{c}{r}\mu$. Therefore, the expected weight
coming into a vertex $v\in R$ would be 

\[ \E[X_v] = \sum_{u\in L} \E[X_{uv}] = \frac{cl\mu}{r} = ck\mu\]

However, $X_{uv}$ for each $u$ are i.i.d random variables. Since by
assumption $ck\mu = 1+\epsilon$, by a Hoeffding bound we can obtain:

\[ \Pr[X_v \leq 1] = \Pr[X_v - \E[X_v] \geq \epsilon] \leq \exp\left(-\frac{2l\epsilon^2}{b^2}\right) \]

By linearity of expectation we can now get the result in the theorem

\[ \E[S] = \sum_{v\in R} \E[X_v] = r\left(1-\exp\left(-\frac{2l\epsilon^2}{b^2}\right)\right) \]
\end{proof}

There are two things to note about this variant. The first is that
since the variables $X_v$ are negatively correlated, our results in
\ref{worst-vs-avg} can be readily extended to the results of this
section. The second is that the condition that $W_{uv}$ are i.i.d
is not necessary to obtain the full effect of the analysis. Indeed,
the only place in the proof where the fact that $W_{uv}$ are i.i.d
is when we argued that $X_{uv}$ is large with high probability by a
Hoeffding bound. For the bound to apply, it's sufficient to assume
that $W_{uv}$ for all $v$ are independent. In particular, it's 
possible that $W_{uv}$ for all $u$ are inter-dependent. This allows
us to assume an weight distribution that depends on the strength of 
the recommender and the relevance of the recommendation separately.
